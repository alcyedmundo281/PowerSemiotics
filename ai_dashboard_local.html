<!doctype html>
<html lang="es">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>AI Dashboard (Local) | PowerSemiotics</title>
    <!-- Google font to match the rest of the site -->
    <link href="assets/tailwind.css" rel="stylesheet" />
  </head>
  <body class="font-sans">
    <header>
      <h1>AI Dashboard (Local)</h1>
      <!-- Updated navigation bar. Add this page to your existing site navigation. -->
      <nav class="nav">
        <a href="index.html">Inicio</a>
        <a href="ai_dashboard.html">AI Dashboard (API)</a>
        <a href="ai_dashboard_local.html">AI Dashboard (Local)</a>
      </nav>
    </header>
    <div class="container">
      <h2>Herramientas de Aprendizaje Generativo (Local)</h2>
      <p>
        Este panel utiliza <strong>WebGPU</strong> para ejecutar un modelo de
        lenguaje directamente en su navegador, sin enviar los datos a ningún
        servidor. Su navegador debe soportar WebGPU para un rendimiento óptimo.
      </p>
      <textarea
        id="userInput"
        placeholder="Ingrese aquí el texto o pregunta..."
      ></textarea>
      <select id="task">
        <option value="summarize">Resumir</option>
        <option value="translate">Traducir (al inglés)</option>
        <option value="question">Responder preguntas</option>
        <option value="example">Generar ejemplos</option>
      </select>
      <button id="generateBtn">Generar</button>
      <p id="loadingStatus"></p>
      <output id="output"></output>
    </div>
    <!-- Use a module script so we can import WebLLM via CDN -->
    <script type="module">
      /**
       * This script demonstrates how to use WebLLM to run a large language model
       * locally in the browser. It lazily loads the model when needed and then
       * handles user requests by constructing appropriate prompts based on
       * selected tasks. The underlying engine exposes an OpenAI-style API.
       */
      import * as webllm from 'https://esm.run/@mlc-ai/web-llm';

      // Global variables to track engine state
      let engine;
      let engineReady = false;
      const statusEl = document.getElementById('loadingStatus');

      // Pre-fill the user input from a "text" query parameter, if provided.
      // This allows topic pages to pass content directly into the dashboard.
      const params = new URLSearchParams(window.location.search);
      const prefill = params.get('text');
      if (prefill) {
        // URLSearchParams handles percent-encoding, but replace + with spaces for safety.
        const decoded = decodeURIComponent(prefill.replace(/\+/g, ' '));
        const inputEl = document.getElementById('userInput');
        if (inputEl) inputEl.value = decoded;
      }

      /**
       * Load the local model into the WebLLM engine. This may take time on the
       * first run because the model weights need to be downloaded. Progress
       * updates will be displayed in the status element.
       *
       * @param {string} modelId - Identifier of the model to load
       */
      async function loadModel(modelId = 'Llama-3.2-1B-Instruct-q4f32_1-MLC') {
        // If the engine is already loaded, skip reloading
        if (engineReady) return;
        statusEl.textContent =
          'Descargando y cargando el modelo, por favor espere...';
        // Create a new engine instance
        engine = new webllm.MLCEngine();
        // Provide a callback to receive progress reports
        engine.setInitProgressCallback((report) => {
          // report.text provides human-readable status
          statusEl.textContent = report.text;
        });
        try {
          // Reload downloads and initializes the model. You can adjust
          // optional parameters like temperature, top_p, etc. via the config.
          await engine.reload(modelId);
          engineReady = true;
          statusEl.textContent = 'Modelo cargado exitosamente.';
        } catch (err) {
          statusEl.textContent = 'Error al cargar el modelo: ' + err.message;
          throw err;
        }
      }

      /**
       * Construct a system and user prompt based on the selected task. The
       * system prompt tells the model how to behave, while the user prompt
       * contains the actual input text.
       *
       * @param {string} input - Text entered by the user
       * @param {string} task - Selected task (summarize, translate, question, example)
       * @returns {Array<Object>} - An array of messages formatted for WebLLM
       */
      function buildMessages(input, task) {
        let systemContent;
        let userContent;
        switch (task) {
          case 'summarize':
            systemContent =
              'Eres un asistente útil que resume textos en español en resúmenes breves y concisos.';
            userContent = 'Resume el siguiente texto:\n' + input;
            break;
          case 'translate':
            systemContent =
              'Eres un asistente útil que traduce textos del español al inglés.';
            userContent = 'Traduce al inglés el siguiente texto:\n' + input;
            break;
          case 'question':
            systemContent =
              'Eres un asistente conocedor que responde preguntas de manera clara y precisa.';
            userContent = input;
            break;
          case 'example':
            systemContent =
              'Eres un asistente creativo que genera ejemplos o analogías para ilustrar conceptos.';
            userContent =
              'Genera dos ejemplos para ilustrar el siguiente concepto:\n' +
              input;
            break;
          default:
            systemContent = 'Eres un asistente útil.';
            userContent = input;
        }
        return [
          { role: 'system', content: systemContent },
          { role: 'user', content: userContent },
        ];
      }

      /**
       * Call the local model to generate a response based on the input and task.
       *
       * @param {string} input - The user-provided text or question
       * @param {string} task - Task name selected by the user
       * @returns {Promise<string>} - The model's textual response
       */
      async function callLocalModel(input, task) {
        // Ensure the engine has been loaded (lazy load on first request)
        if (!engineReady) {
          await loadModel();
        }
        const messages = buildMessages(input, task);
        const reply = await engine.chat.completions.create({ messages });
        return reply.choices[0].message.content.trim();
      }

      // Bind event handler to the Generate button
      document
        .getElementById('generateBtn')
        .addEventListener('click', async () => {
          const text = document.getElementById('userInput').value.trim();
          const task = document.getElementById('task').value;
          const outputEl = document.getElementById('output');
          if (!text) {
            outputEl.textContent = 'Por favor, ingrese un texto o pregunta.';
            return;
          }
          outputEl.textContent = 'Generando...';
          try {
            const result = await callLocalModel(text, task);
            outputEl.textContent = result;
          } catch (error) {
            outputEl.textContent = 'Error: ' + error.message;
          }
        });
    </script>
  </body>
</html>
