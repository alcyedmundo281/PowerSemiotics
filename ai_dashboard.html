<!doctype html>
<html lang="es">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>AI Dashboard | PowerSemiotics</title>
    <!-- Google font to match the rest of the site -->
    <link href="assets/tailwind.css" rel="stylesheet" />
  </head>
  <body class="font-sans">
    <header>
      <h1>AI Dashboard</h1>
      <!-- Updated navigation bar. Add this page to your existing site navigation. -->
      <nav class="nav">
        <a href="index.html">Inicio</a>
        <a href="ai_dashboard.html">AI Dashboard</a>
      </nav>
    </header>
    <div class="container">
      <h2>Herramientas de Aprendizaje Generativo</h2>
      <p>
        Este panel utiliza <strong>WebGPU</strong> para ejecutar un modelo de
        lenguaje directamente en su navegador, sin enviar los datos a ningún
        servidor. Su navegador debe soportar WebGPU para un rendimiento óptimo.
      </p>
      <textarea
        id="userInput"
        placeholder="Ingrese aquí el texto o pregunta..."
      ></textarea>
      <select id="task">
        <option value="summarize">Resumir</option>
        <option value="translate">Traducir (al inglés)</option>
        <option value="question">Responder preguntas</option>
        <option value="example">Generar ejemplos</option>
      </select>
      <button id="generateBtn">Generar</button>
      <p id="loadingStatus"></p>
      <output id="output"></output>
    </div>
    <!-- Use a module script so we can import WebLLM via CDN -->
    <script type="module">
      /**
       * This script demonstrates how to use WebLLM to run a large language model
       * locally in the browser. It lazily loads the model when needed and then
       * handles user requests by constructing appropriate prompts based on
       * selected tasks. The underlying engine exposes an OpenAI‑style API.
       */
      import * as webllm from 'https://esm.run/@mlc-ai/web-llm';

      // Global variables to track engine state
      let engine;
      let engineReady = false;
      const statusEl = document.getElementById('loadingStatus');
      let articleMode = false;
      const ARTICLE_SYSTEM_PROMPT =
        'Eres un asistente experto. El siguiente texto es de un artículo. Responde las preguntas del usuario sobre este contenido.';

      // Pre-fill the user input from query parameters, including article context.
      const params = new URLSearchParams(window.location.search);
      const contextFlag = params.get('context');
      const prefill = params.get('text');
      const inputEl = document.getElementById('userInput');
      const taskEl = document.getElementById('task');
      if (contextFlag === 'true') {
        const articleText = sessionStorage.getItem('articleContext');
        if (articleText && inputEl && taskEl) {
          inputEl.value = `${articleText}\n\nPregunta: `;
          taskEl.value = 'question';
          articleMode = true;
        }
        sessionStorage.removeItem('articleContext');
      } else if (prefill && inputEl) {
        // URLSearchParams handles percent-encoding, but replace + with spaces for safety.
        const decoded = decodeURIComponent(prefill.replace(/\+/g, ' '));
        inputEl.value = decoded;
      }

      /**
       * Load the local model into the WebLLM engine. This may take time on the
       * first run because the model weights need to be downloaded. Progress
       * updates will be displayed in the status element.
       *
       * @param {string} modelId - Identifier of the model to load
       */
      async function loadModel(modelId = 'Llama-3.2-1B-Instruct-q4f32_1-MLC') {
        // If the engine is already loaded, skip reloading
        if (engineReady) return;
        statusEl.textContent =
          'Descargando y cargando el modelo, por favor espere...';
        // Create a new engine instance
        engine = new webllm.MLCEngine();
        // Provide a callback to receive progress reports
        engine.setInitProgressCallback((report) => {
          // report.text provides human-readable status
          statusEl.textContent = report.text;
        });
        try {
          // Reload downloads and initializes the model. You can adjust
          // optional parameters like temperature, top_p, etc. via the config.
          await engine.reload(modelId);
          engineReady = true;
          statusEl.textContent = 'Modelo cargado exitosamente.';
        } catch (err) {
          statusEl.textContent = 'Error al cargar el modelo: ' + err.message;
          throw err;
        }
      }

      /**
       * Construct a system and user prompt based on the selected task. The
       * system prompt tells the model how to behave, while the user prompt
       * contains the actual input text.
       *
       * @param {string} input - Text entered by the user
       * @param {string} task - Selected task (summarize, translate, question, example)
       * @returns {Array<Object>} - An array of messages formatted for WebLLM
       */
      function buildMessages(input, task) {
        let systemContent;
        let userContent;
        if (articleMode && task === 'question') {
          systemContent = ARTICLE_SYSTEM_PROMPT;
          userContent = input;
        } else {
          switch (task) {
            case 'summarize':
              systemContent =
                'Eres un asistente útil que resume textos en español en resúmenes breves y concisos.';
              userContent = 'Resume el siguiente texto:\n' + input;
              break;
            case 'translate':
              systemContent =
                'Eres un asistente útil que traduce textos del español al inglés.';
              userContent = 'Traduce al inglés el siguiente texto:\n' + input;
              break;
            case 'question':
              systemContent =
                'Eres un asistente conocedor que responde preguntas de manera clara y precisa.';
              userContent = input;
              break;
            case 'example':
              systemContent =
                'Eres un asistente creativo que genera ejemplos o analogías para ilustrar conceptos.';
              userContent =
                'Genera dos ejemplos para ilustrar el siguiente concepto:\n' +
                input;
              break;
            default:
              systemContent = 'Eres un asistente útil.';
              userContent = input;
          }
        }
        return [
          { role: 'system', content: systemContent },
          { role: 'user', content: userContent },
        ];
      }

      /**
       * Call the local model to generate a response based on the input and task.
       *
       * @param {string} input - The user-provided text or question
       * @param {string} task - Task name selected by the user
       * @returns {Promise<string>} - The model's textual response
       */
      async function callLocalModel(input, task) {
        // Ensure the engine has been loaded (lazy load on first request)
        if (!engineReady) {
          await loadModel();
        }
        const messages = buildMessages(input, task);
        const reply = await engine.chat.completions.create({ messages });
        return reply.choices[0].message.content.trim();
      }

      // Bind event handler to the Generate button
      document
        .getElementById('generateBtn')
        .addEventListener('click', async () => {
          const text = document.getElementById('userInput').value.trim();
          const task = document.getElementById('task').value;
          const outputEl = document.getElementById('output');
          if (!text) {
            outputEl.textContent = 'Por favor, ingrese un texto o pregunta.';
            return;
          }
          outputEl.textContent = 'Generando...';
          try {
            const result = await callLocalModel(text, task);
            outputEl.textContent = result;
          } catch (error) {
            outputEl.textContent = 'Error: ' + error.message;
          }
        });
    </script>
  </body>
</html>
