<!doctype html>
<html lang="es">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>AI Dashboard (Local) | PowerSemiotics</title>
    <!-- Google font to match the rest of the site -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700;800&display=swap"
      rel="stylesheet"
    />
    <style>
      /*
         * This page reuses much of the styling from the original dashboard
         * and adds a small status area to display model loading progress.
         */
      body {
        font-family: 'Inter', sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f8fafc;
        color: #1f2937;
      }
      header {
        background-color: #ffffff;
        padding: 1rem 2rem;
        display: flex;
        align-items: center;
        justify-content: space-between;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
      }
      header h1 {
        font-size: 1.5rem;
        margin: 0;
      }
      .nav {
        display: flex;
        gap: 1rem;
      }
      .nav a {
        color: #1f2937;
        text-decoration: none;
        font-weight: 600;
      }
      .nav a:hover {
        color: #4f46e5;
      }
      .container {
        max-width: 800px;
        margin: 2rem auto;
        padding: 0 1rem;
      }
      textarea {
        width: 100%;
        height: 150px;
        padding: 0.5rem;
        font-size: 1rem;
        border-radius: 4px;
        border: 1px solid #e5e7eb;
        resize: vertical;
      }
      select,
      button {
        padding: 0.5rem;
        font-size: 1rem;
        margin-top: 0.75rem;
      }
      button {
        background-color: #4f46e5;
        color: #ffffff;
        border: none;
        border-radius: 4px;
        cursor: pointer;
        transition: background-color 0.2s ease;
      }
      button:hover {
        background-color: #4338ca;
      }
      output {
        margin-top: 1rem;
        display: block;
        white-space: pre-wrap;
        border: 1px solid #e5e7eb;
        padding: 0.75rem;
        border-radius: 4px;
        min-height: 100px;
        background-color: #ffffff;
      }
      /* Loading status styling */
      #loadingStatus {
        margin-top: 1rem;
        font-size: 0.9rem;
        color: #374151;
      }
    </style>
  </head>
  <body>
    <header>
      <h1>AI Dashboard (Local)</h1>
      <!-- Updated navigation bar. Add this page to your existing site navigation. -->
      <nav class="nav">
        <a href="index.html">Inicio</a>
        <a href="ai_dashboard.html">AI Dashboard (API)</a>
        <a href="ai_dashboard_local.html">AI Dashboard (Local)</a>
      </nav>
    </header>
    <div class="container">
      <h2>Herramientas de Aprendizaje Generativo (Local)</h2>
      <p>
        Este panel utiliza <strong>WebGPU</strong> para ejecutar un modelo de
        lenguaje directamente en su navegador, sin enviar los datos a ningún
        servidor. Su navegador debe soportar WebGPU para un rendimiento óptimo.
      </p>
      <textarea
        id="userInput"
        placeholder="Ingrese aquí el texto o pregunta..."
      ></textarea>
      <select id="task">
        <option value="summarize">Resumir</option>
        <option value="translate">Traducir (al inglés)</option>
        <option value="question">Responder preguntas</option>
        <option value="example">Generar ejemplos</option>
      </select>
      <button id="generateBtn">Generar</button>
      <p id="loadingStatus"></p>
      <output id="output"></output>
    </div>
    <!-- Use a module script so we can import WebLLM via CDN -->
    <script type="module">
      /**
       * This script demonstrates how to use WebLLM to run a large language model
       * locally in the browser. It lazily loads the model when needed and then
       * handles user requests by constructing appropriate prompts based on
       * selected tasks. The underlying engine exposes an OpenAI‑style API.
       */
      import * as webllm from 'https://esm.run/@mlc-ai/web-llm';

      // Global variables to track engine state
      let engine;
      let engineReady = false;
      const statusEl = document.getElementById('loadingStatus');

      /**
       * Load the local model into the WebLLM engine. This may take time on the
       * first run because the model weights need to be downloaded. Progress
       * updates will be displayed in the status element.
       *
       * @param {string} modelId - Identifier of the model to load
       */
      async function loadModel(modelId = 'Llama-3.2-1B-Instruct-q4f32_1-MLC') {
        // If the engine is already loaded, skip reloading
        if (engineReady) return;
        statusEl.textContent =
          'Descargando y cargando el modelo, por favor espere...';
        // Create a new engine instance
        engine = new webllm.MLCEngine();
        // Provide a callback to receive progress reports
        engine.setInitProgressCallback((report) => {
          // report.text provides human-readable status
          statusEl.textContent = report.text;
        });
        try {
          // Reload downloads and initializes the model. You can adjust
          // optional parameters like temperature, top_p, etc. via the config.
          await engine.reload(modelId);
          engineReady = true;
          statusEl.textContent = 'Modelo cargado exitosamente.';
        } catch (err) {
          statusEl.textContent = 'Error al cargar el modelo: ' + err.message;
          throw err;
        }
      }

      /**
       * Construct a system and user prompt based on the selected task. The
       * system prompt tells the model how to behave, while the user prompt
       * contains the actual input text.
       *
       * @param {string} input - Text entered by the user
       * @param {string} task - Selected task (summarize, translate, question, example)
       * @returns {Array<Object>} - An array of messages formatted for WebLLM
       */
      function buildMessages(input, task) {
        let systemContent;
        let userContent;
        switch (task) {
          case 'summarize':
            systemContent =
              'Eres un asistente útil que resume textos en español en resúmenes breves y concisos.';
            userContent = 'Resume el siguiente texto:\n' + input;
            break;
          case 'translate':
            systemContent =
              'Eres un asistente útil que traduce textos del español al inglés.';
            userContent = 'Traduce al inglés el siguiente texto:\n' + input;
            break;
          case 'question':
            systemContent =
              'Eres un asistente conocedor que responde preguntas de manera clara y precisa.';
            userContent = input;
            break;
          case 'example':
            systemContent =
              'Eres un asistente creativo que genera ejemplos o analogías para ilustrar conceptos.';
            userContent =
              'Genera dos ejemplos para ilustrar el siguiente concepto:\n' +
              input;
            break;
          default:
            systemContent = 'Eres un asistente útil.';
            userContent = input;
        }
        return [
          { role: 'system', content: systemContent },
          { role: 'user', content: userContent },
        ];
      }

      /**
       * Call the local model to generate a response based on the input and task.
       *
       * @param {string} input - The user-provided text or question
       * @param {string} task - Task name selected by the user
       * @returns {Promise<string>} - The model's textual response
       */
      async function callLocalModel(input, task) {
        // Ensure the engine has been loaded (lazy load on first request)
        if (!engineReady) {
          await loadModel();
        }
        const messages = buildMessages(input, task);
        const reply = await engine.chat.completions.create({ messages });
        return reply.choices[0].message.content.trim();
      }

      // Bind event handler to the Generate button
      document
        .getElementById('generateBtn')
        .addEventListener('click', async () => {
          const text = document.getElementById('userInput').value.trim();
          const task = document.getElementById('task').value;
          const outputEl = document.getElementById('output');
          if (!text) {
            outputEl.textContent = 'Por favor, ingrese un texto o pregunta.';
            return;
          }
          outputEl.textContent = 'Generando...';
          try {
            const result = await callLocalModel(text, task);
            outputEl.textContent = result;
          } catch (error) {
            outputEl.textContent = 'Error: ' + error.message;
          }
        });
    </script>
  </body>
</html>
